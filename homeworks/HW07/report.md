# HW07 – Report

## 1. Datasets

Вы выбрали 3 датасета из 4 (перечислите):

### 1.1 Dataset A

- Файл: `S07-hw-dataset-01.csv`
- Размер: (12000, 9)
- Признаки: числовые
- Пропуски: нет
- "Подлости" датасета: не обнаружил

### 1.2 Dataset B

- Файл: `S07-hw-dataset-02.csv`
- Размер: (8000, 4)
- Признаки: числовые
- Пропуски: нет
- "Подлости" датасета: шум

### 1.3 Dataset C

- Файл: `S07-hw-dataset-03.csv`
- Размер: (15000, 5)
- Признаки: числовые
- Пропуски: нет
- "Подлости" датасета: шум

## 2. Protocol

Исследовал следующие методы кластеризации: KMeans, DBScan, Agglomerative

- Препроцессинг: использовал только `scalling`, тк в выбранных датасетах нет пропуском и категориальных признаков.

<br>

- Поиск гиперпараметров:
  - Для KMeans диапазон k был [2, ..., 20],
  - Для DBScan диапазон eps [1.5, 2.0, 2.5, 3.0, 3.5] и min_samples [3, 5, 10],
  - Для Agglomerative диапазон k [2, ..., 7] и linkages["ward", "complete", "average"].
Эти данные также записаны в файле `artifacts/config.json`
Лучшие гиперпараметры выбирал исходя из лучшей метрики по silhouette

<br>

- Метрики: фиксировал silhouette, Davies-Bouldin, Calinski-Harabasz

<br>

- Визуализация:
  - Для метода KMeans визуализировал графики `silhouette по k` и `inertia по k (elbow heuristic)`,
  - Для метода Agglomerative визуализировал графики `silhouette по k для разных linkage`,
  - График PCA лучшего решения для каждого датасета.

## 3. Models

Для каждого датасета подбирал лучшие гиперпараметры (из п. 2) для каждого метода (KMeans, DBSCAN, AgglomerativeClustering). После сравнивал все три метода и выбирал лучший, наиболее подходящий к датасету.

## 4. Results

Для каждого датасета – краткая сводка результатов.

### 4.1 Dataset A

Dataset : "S07-hw-dataset-01.csv"

- Лучший метод: KMeans с параметром k = 2
- Метрики:
  - `silhouette`: 0.5216395622404242,
  - `davies_bouldin`: 0.6853295219054459,
  - `calinski_harabasz`: 11786.95462267153


### 4.2 Dataset B

Dataset : "S07-hw-dataset-02.csv"

- Лучший метод: Agglomerative с параметром k = 2, linkage = "average"
- Метрики:
  - `silhouette`: 0.41976775922231674,
  - `davies_bouldin`: 0.8791287795996852,
  - `calinski_harabasz`: 395.4825694831712

### 4.3 Dataset C

Dataset : "S07-hw-dataset-03.csv"

- Лучший метод: Agglomerative с параметром k = 2, linkage = "average"
- Метрики:
  - `silhouette`: 0.4252961244445327,
  - `davies_bouldin`: 0.813802016581118,
  - `calinski_harabasz`: 8.943143115556799

## 5. Analysis

### 5.1 Сравнение алгоритмов (важные наблюдения)

- KMeans плохо работает, когда кластеры:
  - имеют разную плотность,
  - содержат выбросы,
  потому что KMeans минимизирует сумму квадратов расстояний до центроидов, задается определенное количество центроидов/кластеров и каждый объект обязан быть отнесён к какому-то кластеру

- Где DBSCAN/иерархическая кластеризация выигрывают и почему?
  - DBScan выигравает при наличии выбросов, при кластерах произвольной формы, когда есть области разной плотности, потому что DBSCAN ищет плотные области, а не центроиды и умеет замечать шум.
  - Иерархическая кластеризация выигрывает при небольшом числе кластеров.

- Сильнее всего на результат повлияло масштабирование

### 5.2 Устойчивость (обязательно для одного датасета)

- Какую проверку устойчивости делали (5 запусков KMeans по разным seed или иной подход)
- Что получилось (в 3-6 строк)
- Вывод: устойчиво/неустойчиво и почему вы так считаете

### 5.3 Интерпретация кластеров

Интерпретация кластеров проводилась на основе анализа средних (и медианных) значений признаков внутри каждого кластера, что позволило выделить различные профили объектов

## 6. Conclusion

Кластеризация - это метод обучения модели без учителя.
В unsupervised-эксперимента нет истинных меток, поэтому мы используем протокол сравнения (препроцессинг → кандидаты → подбор параметров → внутренние метрики → визуализация → интерпретация).
Изучил методы кластеризации: Kmeans, DBScan, Agglomerative.
Научился корректно оценивать качество кластеризации без истинных меток: silhouette_score, davies_bouldin_score, calinski_harabasz_score
Научился делать базовую визуализацию и анализ структуры данных (п. 2)

